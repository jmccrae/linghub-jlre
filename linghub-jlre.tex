\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{todonotes}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Linghub: Harmonizing language resource metadata with linked data}
\titlerunning{Linghub}

\author{John P. McCrae       \and
    Markus Ackermann         \and
    Philipp Cimiano          \and
    Martin Br\"ummer         \and
    Victor Rodr\'iguez Doncel\and
    Daniel Vila              \and
    Gabriela Vulcu           \and
    Andrejs Abele            \and
    Luca Matteis             \and
    Tiziano Flati            \and
    Jorge Gracia}


    \authorrunning{McCrae et al.}

\institute{John P. McCrae, Gabriela Vulcu, Andrejs Abele \at
              Insight Centre for Data Analytics \\
              National University of Ireland Galway \\
              \email{john@mccr.ae, \{gabriela.vulcu,
              andrejs.abele\}@insight-center.org}
           \and
           Markus Ackermann \at
              Institute for Applied Informatics \\
              Leipzig University \\
              \email{ackermann@informatik.uni-leipzig.de}
            \and
            V\'ictor Rodr\'iguez Doncel, Jorge Gracia \at 
                Ontology Engineering Group \\
                Polytechnic University of Madrid \\
                \email{\{vrodriguez, jgracia\}@fi.upm.es}
            \and
            Luca Matteis, Tiziano Flati \at
                University of Rome ``La Sapienza'' \\
                \email{\{matteis, flati\}@di.uniroma1.it}
            \and\\
            John P. McCrae, Philipp Cimiano \at
                Cognitive Interaction Technology, Excellence Cluster \\
                Bielefeld University \\
                \email{cimiano@cit-ec.uni-bielefeld.de}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
\keywords{First keyword \and Second keyword \and More}
\end{abstract}

\section{Introduction}

Most natural language processing systems rely to some extent on the use of
datasets in the form of language resources to train themselves for a specific
language or domain and as such the challenge of adapting a tool requires the
discovery of suitable langauge resources. Unfortunately, language resources are
often published independently by researchers and are thus difficult to discover
with standard information retrieval mechansims (e.g., search engines) and can
quickly become unavailable after a short period of time. There have been a couple of approaches to
record information about language resources and they can be broadly divided into two approaches, which we
term the \emph{curatorial} approach and the \emph{crowd-sourcing} approach. The
curatorial approach relies on the use of expert ``curators'' to document the
existing resources and create high quality \emph{metadata} about the resources
that are available given time and funding for these efforts. This approach has the advantage that it produces
comprehensive descriptions of the resources but has a crucial drawback that it
is difficult for curators to describe, or even know about, all resources that
are available. In contrast, the crowd-sourcing approaches relies on users to
document resources in central catalogues ensuring that there are indexes where
users can easily access these resources. This can provide significantly more
coverage of the language resources that are available but has an issue with
quality of the documentation of the resources is often weak, as the metadata
descriptions are not created by experts but by users in an `ad-hoc' manner.

As both approaches have some clear and significant disadvantages it seems
natural to ask if we can combine the advantages of these approaches.
Thus, we propose an \emph{integral} approach that relies on
collecting metadata about language resources from multiple sources and
integrating them into single records. This relies on the use of state-of-the-art
techniques including word sense disambiguation in order to link the resources
and ensure that they refer to values in the same way. For example, a resource
may have a property called `language' but its value may be a language name in
English or another language or an identifier such as an ISO 639
code~\cite{gordon2005ethnologue} of which there are 3 variants. 

One of the crucial technologies that enables this integration is that of
RDF~\cite{klyne2006resource} and linked data. Linked data is based on four fundamental
principals~\cite{bizer2009linked}.

\begin{enumerate}
    \item Use Uniform Resource Identifiers to identify everything in a resource,
        thus ensuring that every element of the resource can be identified in a
        standard manner.
    \item Furthermore, use HTTP URLs as they require the association with domain
        names, ensuring that the data can
        be clearly traced to its host and thus someone responsible for that
        dataset.
    \item Ensure that URLs resolve, in the sense that when typed in to a web
        browser an appropriate description of the resource is obtained. Ideally
        the server should detect (using content negotiation) the type of the
        user and provide HTML results for humans and XML or JSON for software
        agents.
    \item Provide links to other resources so that it is possible to identify
        commonalities between resources and to handle issues of semantic
        interoperability and provenance.
\end{enumerate}

Linked data makes a very good fit for the task of integrating information about
language resources as it is natural that this would be handled by means of a web
portal and thus stable URLs for resources are easily decided. It is obvious that
HTML descriptions should be provided and in order to meet use cases for
automatic training of NLP systems, we find the provision of a machine-ready API
also of vital importance. Finally, links to other resources are vital to not
only provide links back to the source records, but also to ensure that users can
find out more information about the properties used to describe language resources.

In this paper, we present the Linghub system, which aims to integrate language
resources by means of linked data and thus provide a single accessible portal
for both humans and machines to find suitable language resources or their needs.
This paper is structured as follows: Firstly, in Section~\ref{related-work} we
will discuss some of the existing related work in particular focussing on the
metadata repositories that we will integrate and in
Section~\ref{data-collection} we will describe how we collected the data. In
Section~\ref{modelling} we derive a single data model based on existing
standards that will allow us to combine all the resources and then in
Section~\ref{harmonization} we will show our procedure for harmonizing these
resources. We will describe the web portal in
Section~\ref{linked-data-interface}. In Section~\ref{evaluation}, we provide a
thorough evaluation of the system based on real-world queries and thus show the
effectiveness of our approach, and finally we conclude in
Section~\ref{conclusion}.\footnote{Some results in this paper have been
    previously published in the following workshop papers \cite{}. The results
    and methods are expanded and combined in this paper in line with the journal
    guidelines. In addition, Section~\ref{evaluation} provides a novel
evaluation of the system.}

\section{Related Work}
\label{related-work}

\todo{JPM to finish (please help)}

\cite{nilsson2010interoperability,khoo2010merging,nogueras2004metadata}. 

Monolithic vs. linked

In this line, some experts have recommended \cite{brooks2006towards}:

\begin{quote}
``A larger set of ontologies sufficient for particular purposes should
be used instead of a single highly constrained taxonomy of values.''
\end{quote}

META-SHARE~\cite{gavrilidou2012meta},\cite{piperidis2012meta}.

CLARIN, the \emph{CMDI Component
Specification Language}~\cite{broeder2012cmdi}, \cite{van2012semantic}

LRE-Map~\cite{calzolari2012lre} 

OLAC \cite{piperidis2012meta}

SHACHI \cite{tohyama2008shachi}

ISLRN \cite{choukri2012using}



\emph{Open Linguistics Working Group}~\cite{chiarcos2012open} 

\section{Data Collection}
\label{data-collection}

In order to realize the goal of providing comprehensive metadata about language
resources, it is necessary to collect metadata from a wide range of sources. In
parrticular, we chose four main sources, primarily because these resources have
been released under an open license. These resources are:

\begin{description}
    \item[META-SHARE] A resource created by the META-NET project,  the resource
        provides deep and detailed descriptions of language resources that have
        primarily been constructed by hand.
    \item[CLARIN VLO] The Virtual Language Observatory by the CLARIN project is
        a collection of resources drawn from a wide variety of insitutes
        participating in the CLARIN project. In general, the data has been
        manually curated by the individual contributors and only limited
        integration has been made between the resources. As such the records are
        very different in detail and size.
    \item[Datahub.io] This site is an instance of CKAN used primarily to track
        open and linked data. Most of the data is not of relevance to language
        resources and as such we apply filters to extact the relevant datasets.
    \item[LRE-Map] The LRE-Map was collected by participants at several NLP
        conferences in the last few years. Unfortunately, only the data from
        LREC-2014 is available under an open license and the method of collection has
        lead to significant issues with data quality.
\end{description}

In addition, we have a number of other sources that we investigated for the
experiments described in this paper but unfortunately cannot release through the
Linghub portal due to licensing.

\begin{description}
    \item[LRE-Map] Several other conferences of data are accessible on the Web
        and we have scraped the relevant data.
    \item[OLAC] The Open Language Archives Community collects a large amount of
        data, but clearly states that its own data is not ``open''. Fortunately
        most of the data is also available from CLARIN and other sources.
    \item[ELRA/LDA] We also experimented partially with the catalogue of
        resources provided by theses commercial data providers.
\end{description}

In the following section we describe the format of the resources and the
difficulty in consolidating them with our model.

\begin{table}
\resizebox{0.94\textwidth}{!}{\begin{minipage}{\textwidth}
    \begin{tabular}{l|r|r|r}
        \thead{Source} & \thead{Records} & \thead{Triples} & \thead{Triples per \\ Record} \\
        \hline                                              
        META-SHARE &   2,442 &   464,572 & 190.2 \\
        CLARIN     & 144,570 & 3,381,736 &  23.4 \\
        Datahub.io &     218 &    10,739 &  49.3 \\
        LRE-Map    &   5,712 &    79,576 &  13.9 \\
    \end{tabular}
\end{minipage}}
    \caption{\label{tab:resource-sizes}The sizes of the resources in terms of
    number of metadata records and total data size}
\end{table}

\subsection{META-SHARE}

META-SHARE is provided primarily in a format described in
\cite{gavrilidou2012meta}, which is
and XML format and contains \textbf{XXX}\todo{Fix} elements and as such is a highly complex XML
format. We developed a custom invertible framework called LIXR (pronounced
`elixir')~\footnote{http://github.com/liderproject/lixr}, which allows us to easily and
quickly define the conversion between META-SHARE and RDF. As a great many of the
elements defined in the META-SHARE schema are very unique many of the elements
have to be created in a new ontology, which we developed as the META-SHARE
ontology, which is described in \cite{todo}. 

\subsection{CLARIN}

\begin{table}
\begin{tabular}{l|lc}
Component Root Tag & Institutes & Frequency \\
\hline
Song & 1 (MI) & 155,403 \\
Session & 1 (MPI) & 128,673 \\
OLAC-DcmiTerms & 39 & 95,370 \\
MODS & 1 (Utrecht)& 64,632 \\
DcmiTerms & 2 (BeG,HI) & 46,160 \\
SongScan & 1 (MI) & 28,448 \\
media-session-profile & 1 (Munich) & 22,405 \\
SourceScan & 1 (MI) & 21,256 \\
Source & 1 (MI) & 16,519 \\
teiHeader & 2 (BBAW, Copenhagen) & 15,998 \\
\end{tabular}
\

    \caption{\label{tab:clarin-resources}The relative number of resources in
    each of the schemas used by CLARIN}
\end{table}

CLARIN is also an XML format and is based on the CMDI metadata infrastructure as
defined by \cite{broeder2012cmdi}. This consists of a small shared amount of information and
a specific schema, which is normally unique to the data provider, with the
exception of Dublin Core metadata which is in two common schemas. The total size
and applicability of each of the schemas is given in Table
\ref{tab:clarin-resources} and we have developed export scripts for all of the
top 10 formats.

\subsection{LRE-Map}

The LRE-Map is described in \cite{calzolari2012lre} and is available partly as RDF, in
particular the data from LREC-2014 is available. Unfortunately, the integration
was not trivial as there were errors in the RDF~\cite{del2014lre}, in particular the use of
non-resolving URI schemes that had to be corrected. The older data is also
available on the web site and we obtained it by scraping the web site but were
advised that this data is not available under any open license.

\subsection{Datahub.io}

Datahub.io is an instantiation of the CKAN
software\footnote{\url{http://ckan.org}} and as such can
easily be accessed through the API and the RDF version of each metadata using
the DCAT vocabulary can be accessed. As such the import of this data is quick
and can be done at regular intervals.

\subsection{Others}

In addition, we looked at three other sources that cannot be included in the
public release of Linghub due to the licensing issues. These are OLAC, which
uses an XML format and much like CLARIN this format uses different schemas for
different data producers. Secondly, there are the catalogues of ELRA and LDC
both of which are... \todo{.... I can't remember.}

\section{Modelling}
\label{modelling}

As the basis of the modelling for Linghub we took the DCAT
vocabulary~\cite{maali2014data}, which we is based around the concept of a \emph{dataset}
which has obvious equivalence to many of the elements in the resources we
studied. In addition, DCAT models distributions, i.e., downloads, and catalogues
and we imported each of these elements. We found that some distinctions made in
DCAT, most notably the distinction between access URLs and download URLs, that
give the link to the dataset's home page and the direct link to the data was not
clear in any of our sources and this will continue to be a major stumbling block
to providing fully automatic access to datasets. 

The DCAT model, however, only provides for generic descriptions of datasets and
we wished to capture specific elements that would be of interest to linguists.
As such we worked on developing an extension of DCAT based on the META-SHARE
model, which we call the META-SHARE ontology. This resource is described in
\cite{mccrae2015ontology}, and for the benefit of readers we briefly recap the
model here.

DCAT consists of a \emph{catalog} composed of \emph{datasets}, with a
\emph{catalog record}, which
corresponds to the META-SHARE metadata info element. META-SHARE contains a much
richer description of many aspects than DCAT including contact details, version
information, validation and proposed and actual usage of the dataset. These
elements, when available were directly added to the model. In many cases, basic
properties in the META-SHARE ontology, such as the language of a resource, were
to be found nested under several layers of tags and in such cases we added
property chain links so that they would be more compatible with other resources.
\todo{Give an example}
In addition, META-SHARE contains significant resource type specific information
that is defined by the type of the language resource: one of \emph{corpus},
\emph{tool/service}, \emph{language description} or \emph{lexical conceptual
resource}. These extra elements include media type (text, audio, video or image)
and the encoding of information, formats, classifications, and so forth.

In addition to aligning the META-SHARE model to DCAT and Dublin Core, the
META-SHARE ontology improved on the original model in the following ways:

\begin{itemize}
    \item Removal of the {\tt Info} suffix from the names of  wrapping elements of
        components.
    \item Improvement of names that created confusion, as already noted by the
        META-SHARE group and/or the LD4LT group; thus, {\tt resourceInfo} was renamed
        {\tt LanguageResource}, {\tt restrictionsOfUse} became {\tt
        conditionsOfUse}.
    \item Generalization of concepts, e.g. {\tt
        not\-Available\-Through\-Metashare} with {\tt
        avai\-lable\-Through\-Other\-Distributor};
\item Development of novel classes based on existing values, for example:
    \\$\mathtt{Corpus} \equiv \exists \mathtt{resourceType}.\mathtt{corpus}$
\item Grouping similar elements under novel superclasses, e.g. {\tt
    annotationType} and {\tt genre} values are structured in classes and
    subclasses better reflecting the relation between them. Indicatively, the superclass
    {\tt SemanticAnnotation} can be used to bring together semantic annotation types,
    such as semantic roles, named entities, polarity, and semantic relations.
\item Extension of existing classes with new values and new properties
    (e.g. {\tt licenseCategory} for licences).
\end{itemize}

\section{Harmonization}

\label{harmonization}

Due to the variety of source from which we are obtaining metadata it is
inevitable that there are differences between them. Moreover, the quality of the
resources varies greatly, for example in the case of language META-SHARE uses
ISO 639-3\footnote{\url{http://www-01.sil.org/iso639-3/}}, but a crowd-sourced
resource such as LRE-Map has a wide variety of representations in free text. Our
approach focuses on the properties that are most important for using a resource
including whether the resource resolves, what license it is available under, the
type of the resource (e.g., corpus) and the language or languages covered by the
resources (properties may of course have multiple values).

\subsection{Availability}

\begin{table}
    \begin{center}
	\begin{tabular}{l|cc}
            Format   & Resources  & Percentage\\
		
		\hline                                              
                HTML                &	67,419 & 66.2\%\\
                RDF/XML             &	9,940  & 9.8\% \\
                JPEG Image          &   6,599  & 6.5\% \\
                XML (application)   &	5,626  & 5.6\% \\
                Plain Text          & 4,251    & 4.2\% \\
                PDF                 &	3,641  & 3.6\% \\
                XML (text)          & 3,212    & 3.2\% \\
                Zip Archive         &	801    & 0.8\% \\
                PNG Image           & 207      & 0.2\% \\
                gzip Archive        & 181      & 0.2\% \\
	\end{tabular}
    \end{center}
	\caption{\label{tab:formats}The distribution of the 10 most used formats within the
        analyzed sample of URLs. Note XML is associated with two MIME types.}
\end{table}

Certainly the biggest barrier to re-using a resource is obtaining it and it is
unfortunately the case that many resources become unavailable due to server
failure or similar reasons. We also note that there is an important distinction
that must be made between `access URLs', which is generally a page containing
information and documentation about the resource and generally a download link,
and the `download URL', where the resource can be directly accessed. If we wish
to enable use cases where software agents can autonomously access resources we
would need the latter type of URL, however sadly at the moment nearly all links
given in our sources are `access URLs' and thus we only analyze these links at
the moment.

In our study, we then accessed 119,920 URLs given among our sources of metadata
and we found that 95\% of these resolved successfully (i.e., HTTP Response was
200 OK). We then also analyzed the reported content type of the response, the
results of which are in Table \ref{tab:formats}. We found that text formats, in
particular HTML tended to be the predominate format and we would assume that
these correspond to human-readable pages and not the actual resource in the most
case. A little of 14\% of resources are in a format that seem like data sources
although we observed that many of the RDF/XML results were from source providing
Semantic Web compatible descriptions of resources. In addition, some number of
images were found, which were generally scans of historical documents.

\subsection{Rights}

While accessing a resource is one of the important goals of any research, any
responsible researcher must take into account the license that a resource is
released under. 

\todo{VRD to expand}

\subsection{Usage}

The usage of a language resource is an indication of what purpose it was
created for. Following the example of META-SHARE we distinguish between intended use
and actual use, where intended use is the use by the creator of the resource and
the actual use is another application that has used this resource. As the data
has very little information on the latter case, we focussed primarily on the
intended use, which is recorded clearly in two resources: META-SHARE and
LRE-Map. The taxonomies used in each scheme is different with META-SHARE
defining 83 possible values and LRE-Map suggesting 28 values, while actually
3,985 values have been used. This is due to the collection method of LRE-Map,
which has a dropdown of options or the user can select `other' and enter their
own value. 

For the 28 suggested LRE-Map values we added a manual mapping to the META-SHARE
values and for the rest of the values we developed a mapping algorithm based on
using the Snowball stemmer~\cite{porter2001snowball} and string inclusion match to detect
variants. From a random sample of 100 of such terms we found that 66\% were
correct matches, 16\% were empty fields or non-specific terms (e.g., `various
uses') and 16\% were overly general (e.g., `acquisition'). In addition, we had
one false negative (due to a typo `taggin pos' [sic]) and one novel usage that
was not in META-SHARE (`semantic system evaluation'). As such, we conclude that
the system has 98-99\% accuracy.

\subsection{Language}

\begin{table}
\resizebox{1.1\textwidth}{!}{\begin{minipage}{\textwidth}
    \begin{tabular}{l|c|c}
        Resource   & \thead{Label\\Accuracy}  & \thead{Instance\\Accuracy} \\
%    \renewcommand{\arraystretch}{0.5}\begin{tabular}[x]{@{}c@{}}\tiny
%    Instance\\Accuracy\end{tabular} \\%& Average\\
        \hline                                              
        SIL \small\textit{dice coefficient}  & 81\% & 99.50\% \\%& 0.93 \small\textit{rank}  \\
        SIL \small\textit{levenshtein}  & 72\% & 99.42\% \\%& 0.80 \small\textit{distance} \\
        BabelNet \small\textit{dice coefficient} & \textbf{91\%} & 99.87\% \\%& 0.97 \small\textit{rank} \\
        BabelNet \small\textit{levenshtein} & \textbf{89\%} & 99.85\% \\%& 1.20 \small\textit{distance} \\
        \hline
        SIL + BabelNet & & \\
        \small\textit{dice coefficient} & \textbf{91\%} & 99.87\% \\%& 0.97 \small\textit{rank} \\
        \small\textit{levenshtein} & \textbf{89\%} & 99.85\% \\%& 1.01 \small\textit{distance} \\
    \end{tabular}
    \end{minipage} }
    \caption{\label{tab:language-comparison}Accuracy of language mappings}
\end{table}

We decided to normalize the language identifiers around the ISO 639-3 standard
due to its wide adoption and coverage of nearly all human languages. Many
resources used either this standard already or the shorter two-letter codes from
ISO 639-1 and as such the primary challenge is in fact mapping the names given
in English text. To achieve this we collected lists of language names from the
official SIL
database\footnote{\url{http://www-01.sil.org/iso639-3/download.asp}} and from
BabelNet~\cite{navigli2010babelnet}, a large multi-lingual lexicon.

We compared the results using two string similarity metrics, namely the Dice
Co-efficient and the Levenshtein Distance, the results of which are reported in
Table~\ref{tab:language-comparison}. We measured the accuracy by constructing a
sample of 100 labels and manually mapping them to language codes and we present
the results both as total number of labels matched and weighted by the usage of
those labels by resources. For both resources, we see very high accuracy and the
unmapped labels were mostly for those labels that were very rarely used. 

When deploying this system in the Linghub, however, we did notice that the
system made one very noticable error, namely mapping the label `Greek' to the
language (Muscogee) `Creek' as we only had labels for the language `Modern
Greek'. We thus applied a second scan adding this and a few other common
language name variations.

\subsection{Type}

By the type of resource we mean the form of the resource, such as the basic
categorization of META-SHARE into `Corpus', `Lexical Conceptual Resource', 
`Lexical Description' and `Tool/Service'. For this, we used the properties from
existing resources and applied the Babelfy linking
algorithm~\cite{Moroetal:14tacl}. Once, we
had this result, we manually selected those senses, which corresponded to
language resources, giving us in total 143 categories of which the top 10 were:
Sound', `Corpus',
`Lexicon', `Tool' (software), `Instrumental Music'\footnote{These
resources are in fact recordings of singing in under-resourced languages},
`Service', `Ontology', `Evaluation',
`Terminology' and `Translation software'.


\subsection{Duplicate detection}

\begin{table}
    \begin{center}
    \begin{tabular}{l|cc}
        Resource   & \thead{Duplicate \\ Titles} & \thead{Duplicate \\ URLs} \\
        \hline                                                            
        CLARIN{\tiny~(same contributing institute)}     & 50,589           & 20          \\   
        Datahub.io & 0                & 55             \\
        META-SHARE & 63               & 967            \\
        LRE-Map    & 763              & 454            \\
    \end{tabular}
    \end{center}
    \caption{\label{tab:self-dupes}The number of intra-repository duplicate labels and URLs for
    resources}
\end{table}



\begin{table*}
    \begin{center}
        \begin{tabular}{ll|ccc}
        Resource    & Resource    & Duplicate Titles & Duplicate URLs & Both \\
        \hline                                                                  
        CLARIN      & CLARIN{\tiny~(other contributing institute)}      & 1,202            & 2,884          & 0    \\
        CLARIN      & Datahub.io  & 1                & 0              & 0    \\
        CLARIN      & LRE-Map     & 72               & 64             & 0    \\
        CLARIN      & META-SHARE  & 1,204            & 1,228          & 28    \\
        Datahub.io  & LRE-Map     & 59               & 5              & 0    \\
        Datahub.io  & META-SHARE  & 3                & 0              & 0    \\
        LRE-Map     & META-SHARE  & 91               & 51             & 0    \\
        \hline
        All         & All         & 2,632            & 4,232          & 28   \\
        \end{tabular}
    \end{center}
    \caption{\label{tab:dupes}Number of duplicate inter-repository records by type}
\end{table*}

\begin{table}
    \begin{tabular}{l|ccc}
        Duplication & Correct & Unclear & Incorrect \\
        \hline                    
        Titles      &    86   &   6     &    8      \\ 
        URLs        &    95   &   2     &    3      \\
        Both        &    99   &   1     &    0      \\
    \end{tabular}
    \caption{\label{tab:dupe-precision}Precision of matching strategies from a
    sample of 100}
\end{table}

\begin{table}
    \begin{tabular}{l|cc}
        Property   &  \thead{Record Count\\(As percentage of all records)} & Triples \\
        \hline
        Access URL &  91,615 (91.6\%) & 191,006  \\
        Language   &  50,781 (50.7\%) & 98,267   \\
        Type       &  15,241 (15.2\%) & 17,894   \\
        Rights     &   3,080 (3.0\%)  & 8915     \\
        Usage      &   3,397 (3.4\%)  & 4,530    \\ 
    \end{tabular}
    \caption{\label{tab:total}Number of records and facts harmonized by our
    methods}
\end{table}
        
It is a natural effect of collecting records about resources that we will have
multiple records that in fact describe the same resource. In order to provide a
single view of the description of a dataset for a user, it is important that all
the information can be consolidated so that we can for example, indicate all
uses of a particular dataset. This is particularly the case with LRE-Map were
resources are frequently reported multiple times in different uses. As such, we 
make a fundamental distinction between \emph{inter-repository duplication},
where we have two records from different sources describing the same entity, and
\emph{intra-repository duplication}, where resource are described multiple times
by the same source. Note in the case of CLARIN it is quite common to see
duplication between different contributing instances and these are normally
descriptions of the same resource in different formats, so we treat them as
inter-repository duplicates. In both cases, we based our detection on looking
for duplicate title and URLs for resources.

We will first look at the case of intra-repository duplication, where the causes
seem to be quite different in each of the sources:

\begin{description}
    \item[META-SHARE] The duplicates here were due to errors in the export and
        were easy to correct.
    \item[CLARIN] In many cases sequences of resources had multiple records. For
        example the `Universal Declaration of Human Rights' had an individual
        page for each language and thus we merged resources with the same title,
        as we believe this is of more use to our users.
    \item[Datahub.io] This resource does not allow for duplicate titles, but
        duplicate URLs are quite common, however these are more likely due to
        shared resources (e.g., several resources use the same SPARQL endpoint).
    \item[LRE-Map] Duplicates in LRE-Map are caused by multiple submissions
        using the same resource, and as such we wish to aggregate all these
        citations in order to make it clear how frequently a resource is used
        and thus show the resources' quality
\end{description}

The total number of intra-repository duplicates detected is presented in Table~\ref{tab:self-dupes}.

For the case of inter-repository duplication, we assume that in all cases this
is due to multiple descriptions of the same resource. In order to evaluate the
effectiveness of our method of matching by title and URL we took a sample of 100
resources and examined whether they actually referred to the same resources. The
results of this analysis are given in~\ref{tab:dupe-precision} and the number of
duplicates detected in total is given in~\ref{tab:dupes}.


\section{Linked Data Interface}
\label{linked-data-interface}


\begin{figure*}
\includegraphics[width=\textwidth]{linghub-screenshot.png}
\caption{A screenshot of the Linghub interface\label{fig:screenshot}}
\end{figure*}

\todo{TO REWRITE (published in Semantics)}

In order to enable users to quickly and easily discover datasets, we set up a 
portal for browsing the dataset. Naturally we set this up as a site that publishes
the individual records as either RDF or HTML, with the actual content delivered
to the client decided by means of content negotiation. We developed 
templates that render the RDF in a readable manner, while still appearing close
to the data in such a way that users would get a consistent view of a dataset record even
if it came from a different original source and hence had very different
properties.
In addition, we provide a 
number of mechanisms by which users and automated agents can discover a dataset.
For users, we allowed resources to be discovered by means of faceted browsing by
enabling users to select properties and their values. We fixed the list of
properties in advance to those that have been harmonized so as to not overload
the user with choices for properties that only occur for a few datasets and also
to enable the compilation of indexes to speed up page load times. In addition the
front page of Linghub contains a free-text search engine allowing the users to
query fields by a property. This free-text search engine is powered by a
separate index which includes not only the text of data properties but also the
labels of URIs which appear as the value of object properties. 
Machine-based agents may access
the endpoint by means of SPARQL querying, although the endpoint limits the agents
to a subset of the SPARQL query language. The goal of this is to enable constant
query-time without overloading our server. The nature of SPARQL makes it very
easy for users to write queries that are of a complexity that would not be easy
to answer and other sites have attempted to handle this by enforcing timeouts on
SPARQL queries. In general we find this solution to be sub-optimal as it
means that queries may fail unpredictably if the server has many concurrent
connections. Instead, we limit the complexity of the queries themselves by
requiring that the triples have certain properties that can be easily answered.
These include:

\begin{enumerate}
    \item A required limit on the number of results;
    \item The property may not be a variable, thus limiting the number of
        results;
    \item The query must be a `tree' in that every triples should be connected
        from a single root node.
\end{enumerate}

Furthermore, the SPARQL endpoint also by default returns SPARQL-JSON
results\cite{seaborne2013sparql}, so that the results may be easily applied. This is based on the
fact that many clients, notably client-side Javascript in browsers, will not
accept XML due to security concerns. Other clients may still obtain SPARQL-XML
by supplying the appropriate header or parameter in the query.

\section{Evaluation}
\label{evaluation}

\subsection{Evaluation Methodology}

To evaluate the usefulness of Linghub based on actual needs for linguistic 
resources, all questions for language resources posed on the Corpora Mailing 
List (CML)~\footnote{\url{http://mailman.uib.no/public/corpora/}} since January
1st 2015 have been examined. After ruling out a small subset of questions on the
mailing list that were too underspecified and unclear to be operationalised even
for a completely manual search, a catalogue of 23 request was assembled. Each
request poses a number of constraints on the data being searched. Constraints
typically contain the \emph{type} of the resource like corpus, lexicon, tool or
service; the language the resource is applicable for; the relative size of the
resource measured in various metrics like word count and additional qualities,
like intended use.  

Each request was then searched for in LingHub using both, the standard search
\footnote{To make the search for suitable interface queries more time-efficient,
    a prototype for the query was first developed testing equivalent SQL queries
    against the SQLite database schema backing the YuZu TripleBackend, since for
    example the user interface itself offers no straightforward way to find out
    about the total number of matches for a query without traversing the
pagination}
interface as well as SPARQL queries. This covers both relevant audiences:
Standard users like most linguists and developers looking for data without being
familiar with SPARQL, and Semantic Web experts fluent in SPARQL. Results of the
queries were counted and each was evaluated as relevant, irrelevant or related
\footnote{``related'' meaning that some constraints were not or not fully met.}
to the query. The original request \#6 was skipped, because the evaluators could
not agree what exactly constitutes a ``corpus suitable for training hierarchical
classification models''. 

\subsubsection{Limitations}

The methodology chosen can only evaluate LingHub in a limited way. The first and
most important limitation is the biased sample of queries. People asking for
help on the Corpora Mailing List will most probably already have searched using
standard means, such as Google or repositories known to them. This can limit the
queries to very specific requests, that are complicated to find answers for. The
technical expertise of the users and, consequently, their more specific
requirements also contribute to the requests being more complicated. Thus, all
but a few queries examined are expert level queries, limiting the general
applicability of this evaluation. 
Although the timespan examined constitutes a reasonable sample with six months
of mailing list requests analysed, the number of queries is quite low. However,
as section 2 will detail, their overall type and language profile matches
available LingHub data.
Due to the complicated nature of the requests, translating them from natural
language to concise and reasonably narrow queries to the search interface
respectively SPARQL constitutes another hurdle, that is hard to control for. 
Search was furthermore conducted by two persons, whose interpretation of the
meaning of the query and thus the relevance of the resources found may vary. 

\subsection{Resource request analysis}

The requests were analysed in regard to the constraints they express. All
queries expressed at least 2 constraints, usually regarding the resource type
and the preferred language of the resource. Requested resource types were
limited to corpora, tools, lexical and spoken resources. Corpora were requested
in nearly 70\% of the cases, followed by tools (17.4\%) and lexical resources
(13.0\%). As seen in Figure 1, this resembles the resource distribution in
LingHub, where 71.5\% are corpora as well. 
Most language restrictions asked for one language or made no specification at
all. The falling number of requests for higher language restriction count with a
small rise above 20 languages again resembles the resource distribution in
LingHub, that also shows a power law distribution of resource language count.
The rise above 20 languages can be explained by the use of European corpora that
contain all European languages.  

\todo{Markus: Can you provide original images for this paper?}

\begin{figure}
    markus to upload
    \caption{\label{fig:req-by-type}LingHub resources and resource requests by
type}
\end{figure}

\begin{table}
    \begin{tabular}{c|c}
    Language count constraint & Number of requests \\
    \hline
    1                         & 8                  \\
    2                         & 1                  \\
    3                         & 2                  \\
    4                         & 1                  \\
    \textgreater 20                      & 2                  \\
    unspecified               & 7                  \\
\end{tabular}
    \caption{\label{tab:language-constraints}  Language constraints in requests
        and resources available in LingHub covering specific numbers of
    languages}
\end{table}

\begin{figure}
    markus to upload
    \caption{\label{fig:language-constraints} Language constraints in requests
        and resources available in LingHub covering specific numbers of
    languages}
\end{figure}

Besides these common constraints, most requests also define at least one further
restriction. For corpus requests, a common restriction is having a large size in
number of words. Specifically, “gigaword” corpora or corpora with “billions of
words” are highly searched for. Surprisingly, only one request explicitly
limited the license to free and open resources. Further restrictions mention the
type of annotation (for example manually checked or annotating specific
features), the feature set of a tool or service or the semantic content of the
resource. These restrictions can only be judged by first doing a full-text
search in the description of the resources, then reading these descriptions and
judging their appropriateness. Thus, the assessment of relevance or relatedness
of resources found during search is subjective and may not always be comparable.


\subsection{Standard search interface}

While trying to express the facets of the questions from the CML in Linghub,
several shortcomings both of the browsing functionality and the `simple' search
became apparent. 
The inability to declare combined restrictions for several facets (description,
language, rights, $\cdots$) simultaneously is presumably the most crucial limitation
for both the browsing interface and the search form. Online catalogues usually
either offer alternative search form for `advanced' users where a flexible
number of pairs of field specifiers and a corresponding search patterns can be
defined, whose constraints will be combined by boolean operators or complex
search patterns can be defined, allowing for sub-patterns that are matched
against solely against specified record fields, e.g.:

\begin{verbatim}
(title:corpus OR description:corpus) AND 
  (description:(“part of speech” OR pos)) AND 
  (language:(deu* OR german))
\end{verbatim}

Since the overwhelming majority resource requests from the CML involved ‘soft’
constraints that can only be expressed with full-text patterns against resource
descriptions, additional structured information about language or rights could
not be harnessed during the query (as the sole facet choice already had to be
used for the description). Especially for formulating multiple ‘soft’
constraints against the resource descriptions the full text search capabilities
from SQLite exposed in the search interface proved very useful. 
%These
%possibilities should be advertised more prominently to users\footnote{
%    During the evaluation process one of the evaluators only became aware of the
%    availability of full-text query operators when examining YuZu source code to
%    obtain a better understanding of other aspects of the behaviour of the
%Linghub platform.}.  

Some of the facets offered by Linghub are naturally textual (e.g., title,
description, creator), but others are rather categorial with a limited set of
options (e.g., resource type, language). The usability of the search interface
could benefit if constraint formulation for the categorial facets would be
designed in a different manner as a simple full-text search field, presenting
the set of options and allowing selection of the desired subset of suitable
values for the resource request to be formulated. Such facilities are already
implemented in the browsing interface, but only a sole option can be selected
there at the moment.  

Currently also categorial facets are to be queried with the full-text
facilities, which on the one hand allows for selecting combination of allowed
categorial values, but requires knowledge of appropriate matching string values.
One extreme example of this limitation is the usage of the language facet for
the current implementation: the full-text pattern is matched against the
lexvo-URI representations of the ISO 639-3 language codes. So the search
``language=german'' will yield (perhaps surprisingly) no results. The desired
restriction has to be framed as language=deu, which might be perceived as
unintuitive for uninformed first-time users. 

Currently the search form does not ensure that text pattern are matched
exclusively against title and description values for resources with matching
languages this can be problematic due to homographs and cognates shared by
English, Spanish, French etc.  

Excluding the queries that did not yield results, on average 10.8\% of the
results were relevant, 11.9\% were related and 77.3\% were irrelevant. The high
share of irrelevant search results does not come surprising, as the queries
formulated for evaluation generally were rather open than restrictive in many
cases to favour recall over precision, as one can assume that potential users of
language resources will be willing to invest a bit more time to manually sift
through an acceptable amount of additional false positives rather than risk
missing information of an additional potentially useful resource. 

\begin{figure}
    markus to upload
    \caption{\label{fig:freetext}Percentages of relevant standard interface
    search results}
\end{figure}

\subsection{SPARQL search}

While the standard search interface presents the default access point to the
LingHub knowledge base, SPARQL search is regarded as ``advanced search''. Using
SPARQL naturally solves most issues of the standard search interface, as it can
be used for granular search in arbitrary literal fields and natively provides
logical operators for filters to combine them. If values are modelled as
classes, the user does not have to use string matching but can use object
relations, working on a well-defined, semantic level. The only disadvantage of
SPARQL is its inaccessibility to most linguists that don’t have firm knowledge
in database query languages.  

SPARQL queries were written and executed using the downloaded LingHub dump for
convenience. The queries and detailed results can be found in the appendix.

Figure 4 shows the results for each of the queries. Excluding queries 6 and 18
that yielded no results, on average 21.5\% of the results were relevant, 10.7\%
were related and 67.8\% were irrelevant. Roughly a quarter of the queries
failed, yielding no relevant results or no results at all. In general, this can
be considered as a good result, taking into account the complicated nature of
the queries. Specifically, it indicates that the more granulated means SPARQL
provides to the user lead to more accurate results. 

\begin{figure}
    markus to upload
    \caption{\label{fig:sparql}Percentages of relevant SPARQL search results}
\end{figure}

Language proved to be the most restricting constraint, leading to most irrelvant
results, even if other constraints are met. Although SPARQL was used to
granularly restrict the 
languages by leveraging the \texttt{dc:language}, queries were not limited to
this relation but also incorporated searching for the language name in the
\texttt{dcterms:description} to increase recall. 
Size was another important constraint that was never fully met. If size was a
restriction, it always was in the range of billions of words, excluding all
relevant corpora as too small. Corpora with millions of words were counted as
related. It would be helpful to express this measure as an own datum\todo{I
don't understand this - JPM} in LingHub
to help future searchers. However, it could only be automatically acquired by
retrieving and parsing the resources, taking into account their format. This
approach would thus be limited to open resources with detailed metadata. Again,
description texts were used to retrieve hints on corpus size.
One large advantage of SPARQL was being able to explicitly address fields such
as distribution and license information, allowing to filter for free and open
resources, as well as links to the data itself. The selection of the queries in
CML reduced this use-case to one case, but it seems to be an important and often
overlooked facet of data acquisition. 

\subsection{Data Completeness and Quality}

To obtain statistics on data completeness, relative frequencies of Linghub
resources carrying at least on property-value pair for the various facets
offered by the search frontend were determined. The basic quantity of Linghub
resources was defined as all URIs in the Linghub dataset with the
linghub.lider-project.eu hostname appearing in subject position of at least one
triple. Table \ref{tab:reqinfo} and Figure
\ref{fig:reqinfo} present the aforementioned relative frequencies:


\begin{table}
    \begin{tabular}{ll|cc}
    \multicolumn{2}{c|}{required information} & \multicolumn{2}{c}{fulfilling resources} \\
    UI facet name & property          & abs. freq & rel. freq. \\
    \hline
    (none)        & (none)            & 444606    & 1          \\
    Title         & dc:title          & 191355    & 43.04\%    \\
    Description   & dc:description    & 55697     & 12.53\%    \\
    Language      & dct:language      & 47357     & 10.65\%    \\
    Type          & dct:type          & 12780     &  2.87\%    \\
    Rights        & dc:rights         & 30165     &  6.78\%    \\
    Creator       & dc:creator        & 137675    & 30.97\%    \\
    Subject       & dc:subject        & 44725     & 10.06\%    \\
    Contact Point & dcat:contactPoint & 2436      &  0.55\%    \\
    Access URL    & dcat:accessURL    & 184452    & 41.49\%    \\
    \end{tabular}
    \caption{\label{tab:reqinfo} Portions of Linghub resource instances with at
least one statement for the respective facet/property}
\end{table}

\begin{figure}
        markus to upload
    \caption{\label{fig:reqinfo} Portions of Linghub resource instances with at
least one statement for the respective facet/property}
\end{figure}
 
These statistics reveal significant variation in coverage over the various
facets, that can result in unexpected recall when relying on facets with low
coverage. To illustrate this with an example: 444 Linghub resources containing
keywords `spanish' or `spain' in their description also carry a corresponding
dc:language property. On the other hand 493 resources with aforementioned
keywords in the description do not carry a dc:language attribute. Although the
mere appearance of the keywords are not conclusively indicative that the
resource should be counted to the corresponding language, the majority of the
latter resources appeared to be Spanisch or relevant for Spanish when examining
a 10\% sample. Albeit these resources are currently ruled out implicitly when a
user uses the filter option for language. Combined usage of the language
property values when present and fall-back to text-matching on title and
description, as used in the SPARQL queries, can mitigate this problem. Using the
free text pattern syntax with field specifiers, this procedure can be sketched
as: 

\begin{verbatim}
language:spa OR title:(spanish OR spain*) OR 
  description:(spanish OR spain*)
\end{verbatim}

Analogous text-pattern fallback strategies might be employed for other
properties with low-coverage (e.g. type) and could be offered as on-demand
option in the search interfaces. 

Several of the examined resource requests from the CML asking for corpora also
formulated minimal requirements for their size. About 4000 resources listed in
LingHub carry property-value pairs for numeric values quantifying their size
according to a specified unit. Excluding also cases where the unit is not
clearly specified or where the size value is just a sentinel value for ‘no
available’, about 2490 resources with well defined, structured size information
remain (all of these originate from META-SHARE).  Increasing this coverage
towards a substantial portion of corpus resources indexed in LingHub would be
quite beneficial. However, although missing size information could probably be
extracted from description texts for many resources, achieving a satisfactory
level of correctness of such a process would presumably require a prohibitively
great extend of manual annotation and curation


\section{Conclusion}
\label{conclusion}

\todo{JPM to finish}

%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{linghub-jlre}   % name your BibTeX data base

\end{document}
